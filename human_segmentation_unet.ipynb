{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "human_segmentation_unet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Fgy2I9PKokdh"
      ],
      "toc_visible": true,
      "mount_file_id": "1gA0c1kq8f8QWMevx-67TJjAE03j5urDA",
      "authorship_tag": "ABX9TyOPwEG1YB8KWnxEWHyIDZhP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/detsikas/Semantic-Segmentation/blob/master/human_segmentation_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4cnDc62drOX",
        "colab_type": "text"
      },
      "source": [
        "# Description\n",
        "\n",
        "Based on https://arxiv.org/abs/1505.04597"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBkoYcMSu-KN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import datetime\n",
        "import pandas as pd\n",
        "\n",
        "base_folder = '/content/drive/My Drive/colab_data'\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GibPpYPRsOk2",
        "colab_type": "text"
      },
      "source": [
        "# IO functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfDQDDwOYNWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The last logged epoch is probably not complete. So training will resume from that\n",
        "def get_last_complete_epoch(latest):\n",
        "  _, filename = os.path.split(latest)\n",
        "  epoch_str = re.split('\\.|-', filename)[1]\n",
        "  try:\n",
        "    epoch_num = int(epoch_str)\n",
        "    return epoch_num-1\n",
        "  except ValueError:\n",
        "    print('Bad checkpoing filename formnat: {}'.format(filename))\n",
        "    sys.exit(0)\n",
        "\n",
        "def create_metrics_log_file(output_path):\n",
        "  loop = True\n",
        "  index = 0\n",
        "  while(loop):\n",
        "    csv_logger_path = os.path.join(output_path, 'metrics_{}.log'.format(index))\n",
        "    if not os.path.exists(csv_logger_path):\n",
        "        loop = False\n",
        "    else:\n",
        "        index+=1\n",
        "  return csv_logger_path\n",
        "\n",
        "\n",
        "def write_arguments_to_file(args, output_path):\n",
        "  file = os.path.join(output_path, 'args.txt')\n",
        "\n",
        "  with open(file, 'w') as fp:\n",
        "    for key in args:\n",
        "      value = args[key]\n",
        "      if value is not None:\n",
        "        if not isinstance(value, (bool)):\n",
        "          fp.write(\"--\" + key + \"\\n\")\n",
        "          fp.write(str(value)+\"\\n\")\n",
        "        elif value is True:\n",
        "          fp.write(\"--\" + key + \"\\n\")\n",
        "\n",
        "\n",
        "def read_work_paths(args):\n",
        "  if not os.path.exists(args['dataset_path']):\n",
        "    sys.exit('Dataset path does not exist')\n",
        "\n",
        "  checkpoint_filename = 'cp-{epoch:04d}.ckpt'\n",
        "  last_complete_epoch = 0\n",
        "  latest_checkpoint = None\n",
        "  if args['restore_from'] is not None:\n",
        "    if not os.path.exists(args['restore_from']):\n",
        "      sys.exit('Restore path {} does not exist'.format(args['restore_from']))\n",
        "\n",
        "    checkpoint_path = os.path.join(args['restore_from'], checkpoint_filename)\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "    if not os.path.exists(args['restore_from']):\n",
        "      sys.exit('Cannot find checkpoints at {}'.format(checkpoint_path))\n",
        "\n",
        "    output_path = args['restore_from']\n",
        "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    last_complete_epoch = get_last_complete_epoch(latest_checkpoint)\n",
        "  else:\n",
        "    output_path = os.path.join(base_folder, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
        "    checkpoint_path = os.path.join(output_path, checkpoint_filename)\n",
        "    # Create output directory\n",
        "    os.makedirs(output_path)\n",
        "    write_arguments_to_file(args, output_path)\n",
        "\n",
        "  return output_path, checkpoint_path, last_complete_epoch, latest_checkpoint\n",
        "\n",
        "\n",
        "def write_configuration_to_file(args, output_path):\n",
        "  config_file = os.path.join(output_path, 'config.txt')\n",
        "  f = open(config_file, 'w')\n",
        "  f.write('Configration\\n')\n",
        "  f.write('------------\\n')\n",
        "  f.write('Training data size: {}\\n'.format(args['training_size']))\n",
        "  f.write('Validations data size: {}\\n'.format(args['validation_size']))\n",
        "  f.write('Expansion method: {}\\n'.format(args['expansion_method']))\n",
        "  f.write('Batch size: {}\\n'.format(args['batch_size']))\n",
        "  f.write('Buffer size: {}\\n'.format(args['buffer_size']))\n",
        "  f.write('Epochs: {}\\n'.format(args['epochs']))\n",
        "  f.write('Image size: {}\\n'.format(args['image_size']))\n",
        "  f.write('Dataset path: {}\\n'.format(args['dataset_path']))\n",
        "  f.write('Restore from: {}\\n'.format(args['restore_from']))\n",
        "  f.write('Initializer: {}\\n'.format(args['initializer']))\n",
        "  f.write('Regularizer: {}\\n'.format(args['regularizer']))\n",
        "  f.write('Comments: {}\\n'.format(args['comments']))\n",
        "  f.write('TPU: {}\\n'.format(args['tpu']))\n",
        "  f.close()\n",
        "\n",
        "\n",
        "def create_argument_parser():\n",
        "  parser = argparse.ArgumentParser(fromfile_prefix_chars='@')\n",
        "  parser.add_argument('--dataset_path', help='Dataset path')\n",
        "  parser.add_argument('--epochs', help='Training epochs (default 100)', type=int, default=100)\n",
        "  parser.add_argument('--training_size', help='Training samples (default 2000)', type=int)\n",
        "  parser.add_argument('--validation_size', help='Validation samples (default 200)', type=int)\n",
        "  parser.add_argument('--buffer_size', help='Random shuffling buffer (default 1000)',type=int, default=1000)\n",
        "  parser.add_argument('--batch_size', help='Batch size (default 32)', type=int, default=32)\n",
        "  parser.add_argument('--image_size', help='Image size (default 256)', type=int, default=256)\n",
        "  parser.add_argument('--initializer', help='Kernel initializer', action='store_true')\n",
        "  parser.add_argument('--regularizer', help='Kernel regularizer', action='store_true')\n",
        "  parser.add_argument('--comments', help='Comments')\n",
        "  parser.add_argument('--tpu', help='Using TPU', action='store_true')\n",
        "  parser.add_argument('--restore_from', help='Path to restore from checkpoints')\n",
        "  parser.add_argument('--expansion_method', help='Method for the expansion path (default upsampling)',\n",
        "                      choices=['upsampling', 'tconv'],\n",
        "                      default='upsampling')\n",
        "  return parser\n",
        "\n",
        "def print_configuration(args):\n",
        "  print('Configuration')\n",
        "  print('-------------')\n",
        "  print('Training data size: {}'.format(args['training_size'] if args['training_size'] is not None else \"all\"))\n",
        "  print('Validations data size: {}'.format(args['validation_size'] if args['validation_size'] is not None else \"all\"))\n",
        "  print('Expansion method: {}'.format(args['expansion_method']))\n",
        "  print('Batch size: {}'.format(args['batch_size']))\n",
        "  print('Buffer size: {}'.format(args['buffer_size']))\n",
        "  print('Comments: {}'.format(args['comments']))\n",
        "  print('Tpu: {}'.format(args['tpu']))\n",
        "  print('Initializer: {}'.format(args['initializer']))\n",
        "  print('Regularizer: {}'.format(args['regularizer']))\n",
        "  print('Epochs: {}'.format(args['epochs']))\n",
        "  print('Image size: {}'.format(args['image_size']))\n",
        "  print('Dataset path: {}'.format(args['dataset_path']))\n",
        "  print('Restore from: {}'.format(args['restore_from']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSeHdW-BnLkd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw6f3b-MwAiD",
        "colab_type": "text"
      },
      "source": [
        "# Input parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz1hINu_wGVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Restore a previous execution - Other arguments ignored\n",
        "RESTORE_FROM = '' #@param {type:\"string\"}\n",
        "if RESTORE_FROM=='':\n",
        "  RESTORE_FROM = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnJW4P13rwWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Configuration\n",
        "BUFFER_SIZE = 1000 #@param {type:\"number\"}\n",
        "TRAINING_SIZE = 4000 #@param {type:\"number\"}\n",
        "VALIDATION_SIZE =  1000 #@param {type:\"number\"}\n",
        "BATCH_SIZE = 32 #@param {type:\"number\"}\n",
        "EPOCHS = 100 #@param {type:\"number\"}\n",
        "IMAGE_SIZE = 256 #@param {type:\"number\"}\n",
        "DATASET_PATH = 'train_10000_test_2000' #@param {type:\"string\"}\n",
        "INITIALIZER = False #@param {type:\"boolean\"}\n",
        "REGULARIZER = True #@param {type:\"boolean\"}\n",
        "COMMENTS = 'dropout 0.2' #@param {type:\"string\"}\n",
        "EXPANSION_METHOD = 'upsampling' #@param ['upsampling', 'tconv']\n",
        "TPU = False #@param {type:\"boolean\"}\n",
        "\n",
        "DATASET_PATH = os.path.join(base_folder, DATASET_PATH)\n",
        "\n",
        "if DATASET_PATH=='':\n",
        "  DATASET_PATH = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgy2I9PKokdh",
        "colab_type": "text"
      },
      "source": [
        "# Process arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "335cN2swoz9N",
        "colab_type": "code",
        "outputId": "0fbf2413-40b8-4b47-ce01-bdf9374b1843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "args = {}\n",
        "args['restore_from'] = RESTORE_FROM\n",
        "args['dataset_path'] = DATASET_PATH\n",
        "\n",
        "if args['dataset_path'] is None and args['restore_from'] is None:\n",
        "  sys.exit('Dataset path or restore from must be specified')\n",
        "\n",
        "\n",
        "if args['restore_from'] is not None:\n",
        "  file = os.path.join(args['restore_from'], 'args.txt')\n",
        "  parser = create_argument_parser()\n",
        "  args_from_file = parser.parse_args(['@'+file])\n",
        "  args['buffer_size'] = args_from_file.buffer_size\n",
        "  args['training_size'] = args_from_file.training_size\n",
        "  args['validation_size'] = args_from_file.validation_size\n",
        "  args['batch_size'] = args_from_file.batch_size\n",
        "  args['epochs'] = args_from_file.epochs  # Actually the authors use 100ths of thousands\n",
        "  args['image_size'] = args_from_file.image_size\n",
        "  args['initializer'] = args_from_file.initializer\n",
        "  args['regularizer'] = args_from_file.regularizer\n",
        "  args['comments'] = args_from_file.comments\n",
        "  args['expansion_method'] = args_from_file.expansion_method\n",
        "  args['tpu'] = args_from_file.tpu\n",
        "else:\n",
        "  args['buffer_size'] = BUFFER_SIZE\n",
        "  args['training_size'] = TRAINING_SIZE\n",
        "  args['validation_size'] = VALIDATION_SIZE\n",
        "  args['batch_size'] = BATCH_SIZE\n",
        "  args['epochs'] = EPOCHS  # Actually the authors use 100ths of thousands\n",
        "  args['image_size'] = IMAGE_SIZE\n",
        "  args['initializer'] = INITIALIZER\n",
        "  args['regularizer'] = REGULARIZER\n",
        "  args['comments'] = COMMENTS\n",
        "  args['expansion_method'] = EXPANSION_METHOD\n",
        "  args['tpu'] = TPU\n",
        "\n",
        "print_configuration(args)\n",
        "output_path, checkpoint_path, last_complete_epoch, latest_checkpoint = read_work_paths(args)\n",
        "if (args['restore_from']) is None:\n",
        "  write_configuration_to_file(args, output_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Configuration\n",
            "-------------\n",
            "Training data size: 4000\n",
            "Validations data size: 1000\n",
            "Expansion method: upsampling\n",
            "Batch size: 32\n",
            "Buffer size: 1000\n",
            "Comments: dropout 0.2\n",
            "Tpu: False\n",
            "Initializer: False\n",
            "Regularizer: True\n",
            "Epochs: 100\n",
            "Image size: 256\n",
            "Dataset path: /content/drive/My Drive/colab_data/train_10000_test_2000\n",
            "Restore from: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4x4OL9q_5kZ",
        "colab_type": "text"
      },
      "source": [
        "# Read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfiiR-aR_9xM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read images\n",
        "def load_jpg_image(filename):\n",
        "    image = tf.image.decode_jpeg(tf.io.read_file(filename))\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255.0\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_png_image(filename):\n",
        "    image = tf.image.decode_png(tf.io.read_file(filename), channels=1)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255.0\n",
        "    image = tf.round(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "def read_splits(dataset_path):\n",
        "    training_path = tf.strings.join([dataset_path, 'train2017/*'], separator='/')\n",
        "    validation_path = tf.strings.join([dataset_path, 'val2017/*'], separator='/')\n",
        "\n",
        "    training_splits = tf.data.Dataset.list_files(training_path)\n",
        "    validation_splits = tf.data.Dataset.list_files(validation_path)\n",
        "\n",
        "    return training_splits, validation_splits\n",
        "\n",
        "\n",
        "\n",
        "def get_dataset_split(split_path):\n",
        "    # Read split path images\n",
        "    images_pattern = tf.strings.join([split_path, 'images', '*.jpg'], separator='/')\n",
        "    images = tf.data.Dataset.list_files(images_pattern, shuffle=False).map(load_jpg_image,\n",
        "                                                                           num_parallel_calls=AUTOTUNE).cache()\n",
        "\n",
        "    # Read training annotations\n",
        "    annotations_pattern = tf.strings.join([split_path, 'annotations', '*.png'], separator='/')\n",
        "    annotations = tf.data.Dataset.list_files(annotations_pattern, shuffle=False).map(load_png_image,\n",
        "                                                                                     num_parallel_calls=AUTOTUNE).cache()\n",
        "\n",
        "    # Merge images and annotations\n",
        "    dataset_split = tf.data.Dataset.zip((images, annotations))\n",
        "    return dataset_split\n",
        "\n",
        "\n",
        "def get_datasets(dataset_path, train_size=None, val_size=None):\n",
        "    training_splits, validation_splits = read_splits(dataset_path)\n",
        "\n",
        "    training_dataset = training_splits.interleave(lambda x: get_dataset_split(x),\n",
        "                                                  cycle_length=4,\n",
        "                                                  deterministic=False,\n",
        "                                                  num_parallel_calls=AUTOTUNE)\n",
        "    validation_dataset = validation_splits.interleave(lambda x: get_dataset_split(x),\n",
        "                                                      cycle_length=4,\n",
        "                                                      deterministic=False,\n",
        "                                                      num_parallel_calls=AUTOTUNE)\n",
        "    '''\n",
        "    for split in training_splits:\n",
        "        split_path = os.path.join(dataset_path, 'train2017', split)\n",
        "        if training_dataset is None:\n",
        "            training_dataset = get_dataset_split(split_path)\n",
        "        else:\n",
        "            training_dataset = training_dataset.concatenate(get_dataset_split(split_path))\n",
        "\n",
        "    validation_dataset = None\n",
        "    for split in validation_splits:\n",
        "        split_path = os.path.join(dataset_path, 'val2017', split)\n",
        "        if validation_dataset is None:\n",
        "            validation_dataset = get_dataset_split(split_path)\n",
        "        else:\n",
        "            validation_dataset = validation_dataset.concatenate(get_dataset_split(split_path))\n",
        "    '''\n",
        "    if train_size is not None:\n",
        "        training_dataset = training_dataset.take(train_size)\n",
        "\n",
        "    if val_size is not None:\n",
        "        validation_dataset = validation_dataset.take(val_size)\n",
        "\n",
        "    return training_dataset, validation_dataset\n",
        "\n",
        "\n",
        "train_dataset, val_dataset = get_datasets(args['dataset_path'], args['training_size'], args['validation_size'])\n",
        "train_dataset = train_dataset.batch(args['batch_size'])\n",
        "val_dataset = val_dataset.batch(args['batch_size'])\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGe6QKIjAPBO",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7xJxGnuARJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initializer():\n",
        "  return tf.random_normal_initializer(mean=0.0, stddev=0.01)\n",
        "\n",
        "\n",
        "def regularizer():\n",
        "  return tf.keras.regularizers.l2(0.0001)\n",
        "\n",
        "\n",
        "def downsample(input):\n",
        "  x = tf.keras.layers.MaxPool2D(2)(input)\n",
        "  return tf.keras.layers.ReLU()(x)\n",
        "\n",
        "\n",
        "def conv(input, filters):\n",
        "  x = tf.keras.layers.Conv2D(filters=filters, kernel_size=3, padding='same',\n",
        "                              kernel_initializer=initializer() if args['initializer'] else 'glorot_uniform', \n",
        "                              kernel_regularizer=regularizer() if args['regularizer'] else None)(input)\n",
        "  #x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  return tf.keras.layers.ReLU()(x)\n",
        "\n",
        "\n",
        "def conv_transpose(input, filters):\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=filters, kernel_size=2, strides=2, padding='valid',\n",
        "                                        kernel_initializer=initializer() if args['initializer'] else 'glorot_uniform',\n",
        "                                        kernel_regularizer=regularizer() if args['regularizer'] else None)(input)\n",
        "    #x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    return tf.keras.layers.ReLU()(x)\n",
        "\n",
        "\n",
        "def create_model(args):\n",
        "  input_shape = [args['image_size'], args['image_size'], 3]\n",
        "\n",
        "  # Contracting path\n",
        "  model_input = tf.keras.layers.Input(shape=input_shape)\n",
        "  cx_256 = conv(model_input, 32)\n",
        "  cx_256 = conv(cx_256, 32)\n",
        "  cx_128 = downsample(cx_256)\n",
        "  cx_128 = conv(cx_128, 64)\n",
        "  cx_128 = conv(cx_128, 64)\n",
        "  cx_64 = downsample(cx_128)\n",
        "  cx_64 = conv(cx_64, 128)\n",
        "  cx_64 = conv(cx_64, 128)\n",
        "  cx_32 = downsample(cx_64)\n",
        "  cx_32 = conv(cx_32, 256)\n",
        "  cx_32 = conv(cx_32, 128)\n",
        "\n",
        "  # Expanding path\n",
        "  if args['expansion_method'] == 'upsampling':\n",
        "    ex_64 = tf.keras.layers.UpSampling2D(2)(cx_32)\n",
        "  else:\n",
        "    ex_64 = conv_transpose(cx_32, 128)\n",
        "  ex_64_concat = tf.keras.layers.Concatenate()([cx_64, ex_64])\n",
        "  ex_64_concat = conv(ex_64_concat, 128)\n",
        "  ex_64_concat = conv(ex_64_concat, 64)\n",
        "  if args['expansion_method'] == 'upsampling':\n",
        "    ex_128 = tf.keras.layers.UpSampling2D(2)(ex_64_concat)\n",
        "  else:\n",
        "    ex_128 = conv_transpose(ex_64_concat, 64)\n",
        "  ex_128_concat = tf.keras.layers.Concatenate()([cx_128, ex_128])\n",
        "  ex_128_concat = conv(ex_128_concat, 64)\n",
        "  ex_128_concat = conv(ex_128_concat, 32)\n",
        "  if args['expansion_method'] == 'upsampling':\n",
        "    ex_256 = tf.keras.layers.UpSampling2D(2)(ex_128_concat)\n",
        "  else:\n",
        "    ex_256 = conv_transpose(ex_128_concat, 32)\n",
        "  ex_256_concat = tf.keras.layers.Concatenate()([cx_256, ex_256])\n",
        "  ex_256_concat = conv(ex_256_concat, 32)\n",
        "\n",
        "  # Mapping\n",
        "  m = tf.keras.layers.Conv2D(filters=1, kernel_size=3, activation='sigmoid', padding='same',\n",
        "                            kernel_initializer=initializer() if args['initializer'] else 'glorot_uniform',\n",
        "                                        kernel_regularizer=regularizer() if args['regularizer'] else None)(ex_256_concat)\n",
        "\n",
        "  # Model\n",
        "  model = tf.keras.Model(inputs=model_input, outputs=m)\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "\n",
        "  if args['tpu']:\n",
        "    from tensorflow.contrib.tpu.python.tpu import keras_support\n",
        "    tpu_grpc_url = \"grpc://\"+os.environ[\"COLAB_TPU_ADDR\"]\n",
        "    \n",
        "    #connect the TPU cluster using the address \n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\n",
        "    \n",
        "    #run the model on different clusters \n",
        "    strategy = keras_support.TPUDistributionStrategy(tpu_cluster_resolver)\n",
        "    \n",
        "    #convert the model to run on tpu \n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)  \n",
        "\n",
        "  return model\n",
        "\n",
        "model = create_model(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Uznv9ftkvvF",
        "colab_type": "text"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlkl3yRSkydU",
        "colab_type": "code",
        "outputId": "1b0d251b-7873-4ea0-a283-79d3e1618a9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1, period=10)\n",
        "#csv_logger_path = create_metrics_log_file(output_path)\n",
        "csv_logger_path = os.path.join(output_path, 'metrics.log')\n",
        "csv_logger_callback = tf.keras.callbacks.CSVLogger(csv_logger_path, append=True)\n",
        "tensorboard_log_dir = os.path.join(output_path, 'tensorboard')\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_log_dir, histogram_freq=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnMLlUB41IGM",
        "colab_type": "code",
        "outputId": "5dcffeff-6ed9-47fa-a998-df827af0c185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_0Vk6j_AVVP",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib_pMc7BAYW0",
        "colab_type": "code",
        "outputId": "7ac5c092-5452-4df0-b430-82f171d65023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Check if we should resume from a previous training\n",
        "if args['restore_from'] is not None:\n",
        "  print('Loading model from checkpoints. Resuming from epoch: {}'.format(last_complete_epoch+1))\n",
        "  model.load_weights(latest_checkpoint)\n",
        "\n",
        "# Train\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=args['epochs'],\n",
        "                    initial_epoch=last_complete_epoch, \n",
        "                    callbacks=[cp_callback, csv_logger_callback, tensorboard_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "125/125 [==============================] - 4031s 32s/step - loss: 0.4357 - binary_accuracy: 0.8805 - val_loss: 0.3583 - val_binary_accuracy: 0.8853\n",
            "Epoch 2/100\n",
            " 19/125 [===>..........................] - ETA: 13:51 - loss: 0.3590 - binary_accuracy: 0.8773"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ce83bc4c1ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m history = model.fit(train_dataset, validation_data=val_dataset, epochs=args['epochs'],\n\u001b[1;32m      8\u001b[0m                     \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_complete_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     callbacks=[cp_callback, csv_logger_callback, tensorboard_callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdf0dnpjskKe",
        "colab_type": "text"
      },
      "source": [
        "# Save model and results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LNnPhWtsmHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Training complete')\n",
        "print('Saving metrics')\n",
        "output_json_file = os.path.join(output_path, 'history.json')\n",
        "pd.DataFrame.from_dict(history.history).to_json(output_json_file)\n",
        "print('Saving model')\n",
        "model_folder = os.path.join(output_path, 'saved_model')\n",
        "os.makedirs(model_folder)\n",
        "model_filename = os.path.join(model_folder, 'model')\n",
        "model.save(model_filename)\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpDUTYSEHvbm",
        "colab_type": "text"
      },
      "source": [
        "# Pro# fd# Flush drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PWR58kmHw7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}