{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "human_segmentation_unet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1gA0c1kq8f8QWMevx-67TJjAE03j5urDA",
      "authorship_tag": "ABX9TyMsDKiTyLjjBoBZsHDWp9J4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/detsikas/Semantic-Segmentation/blob/master/human_segmentation_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4cnDc62drOX",
        "colab_type": "text"
      },
      "source": [
        "# Description\n",
        "\n",
        "Based on https://arxiv.org/abs/1505.04597"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBkoYcMSu-KN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import datetime\n",
        "import pandas as pd\n",
        "\n",
        "base_folder = '/content/drive/My Drive/colab_data'\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GibPpYPRsOk2",
        "colab_type": "text"
      },
      "source": [
        "# IO functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfDQDDwOYNWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The last logged epoch is probably not complete. So training will resume from that\n",
        "def get_last_complete_epoch(latest):\n",
        "  _, filename = os.path.split(latest)\n",
        "  epoch_str = re.split('\\.|-', filename)[1]\n",
        "  try:\n",
        "    epoch_num = int(epoch_str)\n",
        "    return epoch_num-1\n",
        "  except ValueError:\n",
        "    print('Bad checkpoing filename formnat: {}'.format(filename))\n",
        "    sys.exit(0)\n",
        "\n",
        "def create_metrics_log_file(output_path):\n",
        "  loop = True\n",
        "  index = 0\n",
        "  while(loop):\n",
        "    csv_logger_path = os.path.join(output_path, 'metrics_{}.log'.format(index))\n",
        "    if not os.path.exists(csv_logger_path):\n",
        "        loop = False\n",
        "    else:\n",
        "        index+=1\n",
        "  return csv_logger_path\n",
        "\n",
        "\n",
        "def write_arguments_to_file(args, output_path):\n",
        "  file = os.path.join(output_path, 'args.txt')\n",
        "\n",
        "  with open(file, 'w') as fp:\n",
        "    for key in args:\n",
        "      value = args[key]\n",
        "      if value is not None:\n",
        "        if not isinstance(value, (bool)):\n",
        "          fp.write(\"--\" + key + \"\\n\")\n",
        "          fp.write(str(value)+\"\\n\")\n",
        "        elif value is True:\n",
        "          fp.write(\"--\" + key + \"\\n\")\n",
        "\n",
        "\n",
        "def read_work_paths(args):\n",
        "  if not os.path.exists(args['dataset_path']):\n",
        "    sys.exit('Dataset path does not exist')\n",
        "\n",
        "  checkpoint_filename = 'cp-{epoch:04d}.ckpt'\n",
        "  last_complete_epoch = 0\n",
        "  latest_checkpoint = None\n",
        "  if args['restore_from'] is not None:\n",
        "    if not os.path.exists(args['restore_from']):\n",
        "      sys.exit('Restore path {} does not exist'.format(args['restore_from']))\n",
        "\n",
        "    checkpoint_path = os.path.join(args['restore_from'], checkpoint_filename)\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "    if not os.path.exists(args['restore_from']):\n",
        "      sys.exit('Cannot find checkpoints at {}'.format(checkpoint_path))\n",
        "\n",
        "    output_path = args['restore_from']\n",
        "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    last_complete_epoch = get_last_complete_epoch(latest_checkpoint)\n",
        "  else:\n",
        "    output_path = os.path.join(base_folder, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
        "    checkpoint_path = os.path.join(output_path, checkpoint_filename)\n",
        "    # Create output directory\n",
        "    os.makedirs(output_path)\n",
        "    write_arguments_to_file(args, output_path)\n",
        "\n",
        "  return output_path, checkpoint_path, last_complete_epoch, latest_checkpoint\n",
        "\n",
        "\n",
        "def write_configuration_to_file(args, output_path):\n",
        "  config_file = os.path.join(output_path, 'config.txt')\n",
        "  f = open(config_file, 'w')\n",
        "  f.write('Configration\\n')\n",
        "  f.write('------------\\n')\n",
        "  f.write('Training data size: {}\\n'.format(args['training_size']))\n",
        "  f.write('Validations data size: {}\\n'.format(args['validation_size']))\n",
        "  f.write('Expansion method: {}\\n'.format(args['expansion_method']))\n",
        "  f.write('Batch size: {}\\n'.format(args['batch_size']))\n",
        "  f.write('Buffer size: {}\\n'.format(args['buffer_size']))\n",
        "  f.write('Epochs: {}\\n'.format(args['epochs']))\n",
        "  f.write('Image size: {}\\n'.format(args['image_size']))\n",
        "  f.write('Dataset path: {}\\n'.format(args['dataset_path']))\n",
        "  f.write('Restore from: {}\\n'.format(args['restore_from']))\n",
        "  f.write('Initializer: {}\\n'.format(args['initializer']))\n",
        "  f.write('Regularizer: {}\\n'.format(args['regularizer']))\n",
        "  f.write('Dropout: {}\\n'.format(args['dropout']))\n",
        "  f.write('Batch_norm: {}\\n'.format(args['batch_norm']))\n",
        "  f.write('Comments: {}\\n'.format(args['comments']))\n",
        "  f.write('TPU: {}\\n'.format(args['tpu']))\n",
        "  f.close()\n",
        "\n",
        "\n",
        "def create_argument_parser():\n",
        "  parser = argparse.ArgumentParser(fromfile_prefix_chars='@')\n",
        "  parser.add_argument('--dataset_path', help='Dataset path')\n",
        "  parser.add_argument('--epochs', help='Training epochs (default 100)', type=int, default=100)\n",
        "  parser.add_argument('--training_size', help='Training samples (default 2000)', type=int)\n",
        "  parser.add_argument('--validation_size', help='Validation samples (default 200)', type=int)\n",
        "  parser.add_argument('--buffer_size', help='Random shuffling buffer (default 1000)',type=int, default=1000)\n",
        "  parser.add_argument('--batch_size', help='Batch size (default 32)', type=int, default=32)\n",
        "  parser.add_argument('--image_size', help='Image size (default 256)', type=int, default=256)\n",
        "  parser.add_argument('--initializer', help='Kernel initializer', action='store_true')\n",
        "  parser.add_argument('--regularizer', help='Kernel regularizer', action='store_true')\n",
        "  parser.add_argument('--dropout', help='Use dropout 0.2', action='store_true')\n",
        "  parser.add_argument('--batch_norm', help='Use batch normalization', action='store_true')\n",
        "  parser.add_argument('--tpu', help='Using TPU', action='store_true')\n",
        "  parser.add_argument('--comments', help='Comments')\n",
        "  parser.add_argument('--restore_from', help='Path to restore from checkpoints')\n",
        "  parser.add_argument('--expansion_method', help='Method for the expansion path (default upsampling)',\n",
        "                      choices=['upsampling', 'tconv'],\n",
        "                      default='upsampling')\n",
        "  return parser\n",
        "\n",
        "def print_configuration(args):\n",
        "  print('Configuration')\n",
        "  print('-------------')\n",
        "  print('Training data size: {}'.format(args['training_size'] if args['training_size'] is not None else \"all\"))\n",
        "  print('Validations data size: {}'.format(args['validation_size'] if args['validation_size'] is not None else \"all\"))\n",
        "  print('Expansion method: {}'.format(args['expansion_method']))\n",
        "  print('Batch size: {}'.format(args['batch_size']))\n",
        "  print('Buffer size: {}'.format(args['buffer_size']))\n",
        "  print('Comments: {}'.format(args['comments']))\n",
        "  print('Tpu: {}'.format(args['tpu']))\n",
        "  print('Initializer: {}'.format(args['initializer']))\n",
        "  print('Regularizer: {}'.format(args['regularizer']))\n",
        "  print('Dropout: {}'.format(args['dropout']))\n",
        "  print('Batch_norm: {}'.format(args['batch_norm']))\n",
        "  print('Epochs: {}'.format(args['epochs']))\n",
        "  print('Image size: {}'.format(args['image_size']))\n",
        "  print('Dataset path: {}'.format(args['dataset_path']))\n",
        "  print('Restore from: {}'.format(args['restore_from']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSeHdW-BnLkd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw6f3b-MwAiD",
        "colab_type": "text"
      },
      "source": [
        "# Input parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz1hINu_wGVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Restore a previous execution - Other arguments ignored\n",
        "RESTORE_FROM = '/content/drive/My Drive/colab_data/2020-05-24_18-22-05' #@param {type:\"string\"}\n",
        "if RESTORE_FROM=='':\n",
        "  RESTORE_FROM = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnJW4P13rwWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Configuration\n",
        "BUFFER_SIZE = 1000 #@param {type:\"number\"}\n",
        "TRAINING_SIZE = 10000 #@param {type:\"number\"}\n",
        "VALIDATION_SIZE =  2000 #@param {type:\"number\"}\n",
        "BATCH_SIZE = 32 #@param {type:\"number\"}\n",
        "EPOCHS = 100 #@param {type:\"number\"}\n",
        "IMAGE_SIZE = 256 #@param {type:\"number\"}\n",
        "DATASET_PATH = 'train_10000_test_2000' #@param {type:\"string\"}\n",
        "INITIALIZER = False #@param {type:\"boolean\"}\n",
        "REGULARIZER = True #@param {type:\"boolean\"}\n",
        "EXPANSION_METHOD = 'upsampling' #@param ['upsampling', 'tconv']\n",
        "DROPOUT = False #@param {type:\"boolean\"}\n",
        "BATCH_NORM = True #@param {type:\"boolean\"}\n",
        "TPU = False #@param {type:\"boolean\"}\n",
        "COMMENTS = 'batch norm' #@param {type:\"string\"}\n",
        "\n",
        "#DATASET_PATH = os.path.join(base_folder, DATASET_PATH)\n",
        "\n",
        "if DATASET_PATH=='':\n",
        "  DATASET_PATH = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-szfBk60GWHL",
        "colab_type": "text"
      },
      "source": [
        "# Fetch dataset from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-JmyAGLGVM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_gz_file = DATASET_PATH+'.tar.gz'\n",
        "dataset_gz_full_gdrive_path = os.path.join(base_folder, dataset_gz_file)\n",
        "print(dataset_gz_file)\n",
        "print(dataset_gz_full_gdrive_path)\n",
        "!cp '{dataset_gz_full_gdrive_path}' .\n",
        "!tar xvf '{dataset_gz_file}'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgy2I9PKokdh",
        "colab_type": "text"
      },
      "source": [
        "# Process arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "335cN2swoz9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {}\n",
        "args['restore_from'] = RESTORE_FROM\n",
        "args['dataset_path'] = DATASET_PATH\n",
        "\n",
        "if args['dataset_path'] is None and args['restore_from'] is None:\n",
        "  sys.exit('Dataset path or restore from must be specified')\n",
        "\n",
        "\n",
        "if args['restore_from'] is not None:\n",
        "  file = os.path.join(args['restore_from'], 'args.txt')\n",
        "  parser = create_argument_parser()\n",
        "  args_from_file = parser.parse_args(['@'+file])\n",
        "  args['buffer_size'] = args_from_file.buffer_size\n",
        "  args['training_size'] = args_from_file.training_size\n",
        "  args['validation_size'] = args_from_file.validation_size\n",
        "  args['batch_size'] = args_from_file.batch_size\n",
        "  args['epochs'] = args_from_file.epochs  # Actually the authors use 100ths of thousands\n",
        "  args['image_size'] = args_from_file.image_size\n",
        "  args['initializer'] = args_from_file.initializer\n",
        "  args['regularizer'] = args_from_file.regularizer\n",
        "  args['dropout'] = args_from_file.dropout\n",
        "  args['batch_norm'] = args_from_file.batch_norm\n",
        "  args['comments'] = args_from_file.comments\n",
        "  args['expansion_method'] = args_from_file.expansion_method\n",
        "  args['tpu'] = args_from_file.tpu\n",
        "else:\n",
        "  args['buffer_size'] = BUFFER_SIZE\n",
        "  args['training_size'] = TRAINING_SIZE\n",
        "  args['validation_size'] = VALIDATION_SIZE\n",
        "  args['batch_size'] = BATCH_SIZE\n",
        "  args['epochs'] = EPOCHS  # Actually the authors use 100ths of thousands\n",
        "  args['image_size'] = IMAGE_SIZE\n",
        "  args['initializer'] = INITIALIZER\n",
        "  args['regularizer'] = REGULARIZER\n",
        "  args['comments'] = COMMENTS\n",
        "  args['dropout'] = DROPOUT\n",
        "  args['batch_norm'] = BATCH_NORM\n",
        "  args['expansion_method'] = EXPANSION_METHOD\n",
        "  args['tpu'] = TPU\n",
        "\n",
        "print_configuration(args)\n",
        "output_path, checkpoint_path, last_complete_epoch, latest_checkpoint = read_work_paths(args)\n",
        "if (args['restore_from']) is None:\n",
        "  write_configuration_to_file(args, output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4x4OL9q_5kZ",
        "colab_type": "text"
      },
      "source": [
        "# Read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfiiR-aR_9xM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read images\n",
        "def load_jpg_image(filename):\n",
        "    image = tf.image.decode_jpeg(tf.io.read_file(filename))\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255.0\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_png_image(filename):\n",
        "    image = tf.image.decode_png(tf.io.read_file(filename), channels=1)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255.0\n",
        "    image = tf.round(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "def read_splits(dataset_path):\n",
        "    training_path = tf.strings.join([dataset_path, 'train2017/*'], separator='/')\n",
        "    validation_path = tf.strings.join([dataset_path, 'val2017/*'], separator='/')\n",
        "\n",
        "    training_splits = tf.data.Dataset.list_files(training_path)\n",
        "    validation_splits = tf.data.Dataset.list_files(validation_path)\n",
        "\n",
        "    return training_splits, validation_splits\n",
        "\n",
        "\n",
        "\n",
        "def get_dataset_split(split_path):\n",
        "    # Read split path images\n",
        "    images_pattern = tf.strings.join([split_path, 'images', '*.jpg'], separator='/')\n",
        "    images = tf.data.Dataset.list_files(images_pattern, shuffle=False).map(load_jpg_image,\n",
        "                                                                           num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    # Read training annotations\n",
        "    annotations_pattern = tf.strings.join([split_path, 'annotations', '*.png'], separator='/')\n",
        "    annotations = tf.data.Dataset.list_files(annotations_pattern, shuffle=False).map(load_png_image,\n",
        "                                                                                     num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    # Merge images and annotations\n",
        "    dataset_split = tf.data.Dataset.zip((images, annotations))\n",
        "    return dataset_split\n",
        "\n",
        "\n",
        "def get_datasets(dataset_path, train_size=None, val_size=None):\n",
        "    training_splits, validation_splits = read_splits(dataset_path)\n",
        "\n",
        "    training_dataset = training_splits.interleave(lambda x: get_dataset_split(x),\n",
        "                                                  cycle_length=4,\n",
        "                                                  deterministic=False,\n",
        "                                                  num_parallel_calls=AUTOTUNE)\n",
        "    validation_dataset = validation_splits.interleave(lambda x: get_dataset_split(x),\n",
        "                                                      cycle_length=4,\n",
        "                                                      deterministic=False,\n",
        "                                                      num_parallel_calls=AUTOTUNE)\n",
        "    \n",
        "    training_dataset = training_dataset.shuffle(args['buffer_size'])\n",
        "\n",
        "    if train_size is not None:\n",
        "        training_dataset = training_dataset.take(train_size)\n",
        "\n",
        "    if val_size is not None:\n",
        "        validation_dataset = validation_dataset.take(val_size)\n",
        "\n",
        "    return training_dataset, validation_dataset\n",
        "\n",
        "\n",
        "train_dataset, val_dataset = get_datasets(args['dataset_path'], args['training_size'], args['validation_size'])\n",
        "train_dataset = train_dataset.batch(args['batch_size'])\n",
        "val_dataset = val_dataset.batch(args['batch_size'])\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGe6QKIjAPBO",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7xJxGnuARJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initializer():\n",
        "  return tf.random_normal_initializer(mean=0.0, stddev=0.01)\n",
        "\n",
        "\n",
        "def regularizer():\n",
        "  return tf.keras.regularizers.l2(0.0001)\n",
        "\n",
        "\n",
        "def downsample(input):\n",
        "  x = tf.keras.layers.MaxPool2D(2)(input)\n",
        "  return tf.keras.layers.ReLU()(x)\n",
        "\n",
        "\n",
        "def conv(input, filters, args):\n",
        "  x = tf.keras.layers.Conv2D(filters=filters, kernel_size=3, padding='same',\n",
        "                              kernel_initializer=initializer() if args['initializer'] else 'glorot_uniform', \n",
        "                              kernel_regularizer=regularizer() if args['regularizer'] else None)(input)\n",
        "  if args['batch_norm']:\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "  if args['dropout']:\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  return tf.keras.layers.ReLU()(x)\n",
        "\n",
        "\n",
        "def conv_transpose(input, filters, args):\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=filters, kernel_size=2, strides=2, padding='valid',\n",
        "                                        kernel_initializer=initializer() if args['initializer'] else 'glorot_uniform',\n",
        "                                        kernel_regularizer=regularizer() if args['regularizer'] else None)(input)\n",
        "  if args['batch_norm']:\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "  if args['dropout']:\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  return tf.keras.layers.ReLU()(x)\n",
        "\n",
        "\n",
        "def create_model(args):\n",
        "  input_shape = [args['image_size'], args['image_size'], 3]\n",
        "\n",
        "  # Contracting path\n",
        "  model_input = tf.keras.layers.Input(shape=input_shape)\n",
        "  cx_256 = conv(model_input, 32, args)\n",
        "  cx_256 = conv(cx_256, 32, args)\n",
        "  cx_128 = downsample(cx_256)\n",
        "  cx_128 = conv(cx_128, 64, args)\n",
        "  cx_128 = conv(cx_128, 64, args)\n",
        "  cx_64 = downsample(cx_128)\n",
        "  cx_64 = conv(cx_64, 128, args)\n",
        "  cx_64 = conv(cx_64, 128, args)\n",
        "  cx_32 = downsample(cx_64)\n",
        "  cx_32 = conv(cx_32, 256, args)\n",
        "  cx_32 = conv(cx_32, 128, args)\n",
        "\n",
        "  # Expanding path\n",
        "  if args['expansion_method'] == 'upsampling':\n",
        "    ex_64 = tf.keras.layers.UpSampling2D(2)(cx_32)\n",
        "  else:\n",
        "    ex_64 = conv_transpose(cx_32, 128, args)\n",
        "  ex_64_concat = tf.keras.layers.Concatenate()([cx_64, ex_64])\n",
        "  ex_64_concat = conv(ex_64_concat, 128, args)\n",
        "  ex_64_concat = conv(ex_64_concat, 64, args)\n",
        "  if args['expansion_method'] == 'upsampling':\n",
        "    ex_128 = tf.keras.layers.UpSampling2D(2)(ex_64_concat)\n",
        "  else:\n",
        "    ex_128 = conv_transpose(ex_64_concat, 64, args)\n",
        "  ex_128_concat = tf.keras.layers.Concatenate()([cx_128, ex_128])\n",
        "  ex_128_concat = conv(ex_128_concat, 64, args)\n",
        "  ex_128_concat = conv(ex_128_concat, 32, args)\n",
        "  if args['expansion_method'] == 'upsampling':\n",
        "    ex_256 = tf.keras.layers.UpSampling2D(2)(ex_128_concat)\n",
        "  else:\n",
        "    ex_256 = conv_transpose(ex_128_concat, 32, args)\n",
        "  ex_256_concat = tf.keras.layers.Concatenate()([cx_256, ex_256])\n",
        "  ex_256_concat = conv(ex_256_concat, 32, args)\n",
        "\n",
        "  # Mapping\n",
        "  m = tf.keras.layers.Conv2D(filters=1, kernel_size=3, activation='sigmoid', padding='same',\n",
        "                            kernel_initializer=initializer() if args['initializer'] else 'glorot_uniform',\n",
        "                                        kernel_regularizer=regularizer() if args['regularizer'] else None)(ex_256_concat)\n",
        "\n",
        "  # Model\n",
        "  model = tf.keras.Model(inputs=model_input, outputs=m)\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "\n",
        "  if args['tpu']:\n",
        "    from tensorflow.contrib.tpu.python.tpu import keras_support\n",
        "    tpu_grpc_url = \"grpc://\"+os.environ[\"COLAB_TPU_ADDR\"]\n",
        "    \n",
        "    #connect the TPU cluster using the address \n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\n",
        "    \n",
        "    #run the model on different clusters \n",
        "    strategy = keras_support.TPUDistributionStrategy(tpu_cluster_resolver)\n",
        "    \n",
        "    #convert the model to run on tpu \n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)  \n",
        "\n",
        "  return model\n",
        "\n",
        "model = create_model(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Uznv9ftkvvF",
        "colab_type": "text"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlkl3yRSkydU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1, period=10)\n",
        "#csv_logger_path = create_metrics_log_file(output_path)\n",
        "csv_logger_path = os.path.join(output_path, 'metrics.log')\n",
        "csv_logger_callback = tf.keras.callbacks.CSVLogger(csv_logger_path, append=True)\n",
        "tensorboard_log_dir = os.path.join(output_path, 'tensorboard')\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_log_dir, histogram_freq=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnMLlUB41IGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_0Vk6j_AVVP",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib_pMc7BAYW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check if we should resume from a previous training\n",
        "if args['restore_from'] is not None:\n",
        "  print('Loading model from checkpoints. Resuming from epoch: {}'.format(last_complete_epoch+1))\n",
        "  model.load_weights(latest_checkpoint)\n",
        "\n",
        "# Train\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=args['epochs'],\n",
        "                    initial_epoch=last_complete_epoch, \n",
        "                    callbacks=[cp_callback, csv_logger_callback, tensorboard_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdf0dnpjskKe",
        "colab_type": "text"
      },
      "source": [
        "# Save model and results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LNnPhWtsmHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Training complete')\n",
        "print('Saving metrics')\n",
        "output_json_file = os.path.join(output_path, 'history.json')\n",
        "pd.DataFrame.from_dict(history.history).to_json(output_json_file)\n",
        "print('Saving model')\n",
        "model_folder = os.path.join(output_path, 'saved_model')\n",
        "os.makedirs(model_folder)\n",
        "model_filename = os.path.join(model_folder, 'model')\n",
        "model.save(model_filename)\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpDUTYSEHvbm",
        "colab_type": "text"
      },
      "source": [
        "# Pro# fd# Flush drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PWR58kmHw7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}